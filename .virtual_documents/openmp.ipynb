


import time
import multiprocessing

def sum_chunk(numbers):
  return sum(numbers)

def sequential_sum(numbers):
  total = 0
  for num in numbers:
    total += num
  return total

def main(numbers, p):
  # Sequential sum
  start_time = time.time()
  sequential_result = sequential_sum(numbers)
  sequential_time = time.time() - start_time

  # Parallel sum
  start_time = time.time()
  #chunk_size = len(numbers) // p
  chunk_size = 3
  chunks = [numbers[i*chunk_size:(i+1)*chunk_size] for i in range(p)]
  #print(chunks)
  with multiprocessing.Pool(processes=p) as pool:
    partial_sums = pool.map(sum_chunk, chunks)
  
  parallel_result = sum(partial_sums)
  parallel_time = time.time() - start_time

  return sequential_result, sequential_time, parallel_result, parallel_time

if __name__ == "__main__":
  numbers = list(range(100000))
  p = 8
  sequential_result, sequential_time, parallel_result, parallel_time = main(numbers, p)

  print("Sequential result:", sequential_result, "time:", sequential_time)
  print("Parallel result:", parallel_result, "time:", parallel_time)












Write this small code

#include <stdio.h>
#include <omp.h>

int main(int argc, char** argv){
    #pragma omp parallel
    {
        printf(“Hello from process: %d\n”, omp_get_thread_num());
    }
    return 0;
}

Set this environment variable before running.

export OMP_NUM_THREADS=4












#include <stdio.h>
#include <omp.h>

int main(int argc, char** argv){
    int thread_id=100;

    #pragma omp parallel private(thread_id)
    {
        thread_id++;
        printf("Hello from process: %d\n", thread_id );
    }

    return 0;
}


#Here Separate instances of thread_id for each task





#include <stdio.h>
#include <omp.h>
#include<iostream>

using namespace std;

static long num_steps = 100000;
double step;
int main() {
 double x, pi, sum=0.0;
 step = 1.0/(double)num_steps;
 for(int i=0; i<num_steps; i++) {
  x = (i+0.5)*step;
  sum += 4.0/(1.0+x*x);
 }
 pi = step*sum;
 cout<<"Pi="<<pi<<endl;
}






#include <stdio.h>
#include <omp.h>
#include<iostream>
using namespace std;

#define NUM_THREADS 1

static long num_steps = 100000;
double step;

int main() {
 int i, nthreads;
 double pi, sum[NUM_THREADS];
 step = 1.0/(double)num_steps;
 omp_set_num_threads(NUM_THREADS);
 #pragma omp parallel
 {
   int i, id, nthrds;
   double x;
   id = omp_get_thread_num();
   nthrds = omp_get_num_threads();
   if(id==0) nthreads = nthrds;
   for(i=id,sum[id]=0.0;i<num_steps; i=i+nthrds) {
     x = (i+0.5)*step;
     sum[id] += 4.0/(1.0+x*x);
   }
 }
 for(i=0, pi=0.0; i<nthreads;i++) pi += step*sum[i];
 cout<<"\nPi="<<pi<<endl;
}


/*
Take away

Here you can see the cyclic distribution of the loop iteration.
 - The sum variable has to be shared. Otherwise once the thread is finished, the values it calculated will be lost.
 - Why you had to use different sum variable for each thread.
 - Use of declaring nthreads outside - Because eventhough you request the number of threads, the environment can give you different number of threads.

*/





#include <stdio.h>
#include <omp.h>

#include<iostream>
using namespace std;

#define NUM_THREADS 2

static long num_steps = 100000;

#define PAD 8 //L1 cache line is 64 byte, double is 8 byte

double step;
int main() {
 int i,nthreads;
 double x, pi, sum[NUM_THREADS][PAD];
 step = 1.0/(double)num_steps;
 omp_set_num_threads(NUM_THREADS);
 #pragma omp parallel
 {
   int i, id, nthrds;
   double x;
   id = omp_get_thread_num();
   nthrds = omp_get_num_threads();
   if(id==0) nthreads = nthrds;
   for(i=id,sum[id][0]=0.0;i<num_steps; i=i+nthrds) {
     x = (i+0.5)*step;
     sum[id][0] += 4.0/(1.0+x*x);
   }
 }
 for(i=0, pi=0.0; i<nthreads;i++)pi += step*sum[i][0];
 cout<<"\nPi="<<pi<<endl;
}












#pragma omp barrier

#pragma omp critical { … }


#include <stdio.h>
#include <stdlib.h>
#include <omp.h>

/**
 * @brief Illustrates the OpenMP barrier synchronisation.
 * @details This application is made of a parallel region, in which two distinct
 * parts are to be executed, separated with a barrier. In each part, threads
 * have to print a message. They will print their second message only when all
 * threads will have printed the first one.
 **/
int main(int argc, char* argv[])
{
	// Use 4 threads when we create a parallel region
	omp_set_num_threads(4);

	// Create the parallel region
	#pragma omp parallel
	{
		// Threads print their first message
		printf("[Thread %d] I print my first message.\n", omp_get_thread_num());

		// Make sure all threads have printed their first message before moving on.
		#pragma omp barrier

		// One thread indicates that the barrier is complete.
		#pragma omp single
		{
			printf("The barrier is complete, which means all threads have printed their first message.\n");
		}

		// Threads print their second message
		printf("[Thread %d] I print my second message.\n", omp_get_thread_num());
	}

	return EXIT_SUCCESS;
}



//critical -> Run code segment one by one by all threads

#include <iostream>
#include <stdio.h>
#include <omp.h>

int main() {
  // shared variable
    int sum_shared = 0;
    #pragma omp parallel
    {
        // private varia~ble
        int sum_local = 0;
        //#pragma omp for nowait
        for (int i = 0; i < 10; ++i) {
            sum_local += i;
        }
        // critical section as we update
        // shared variable/ data
        //std::cout<<"Thread ID   = "<<omp_get_thread_num()<<std::endl;
        printf("Thread ID =  %d\n", omp_get_thread_num());
        #pragma omp critical
        {
            sum_shared += sum_local;
        }
    }
    std::cout << sum_shared << std::endl;
  return 0;
}









#pragma omp parallel
 {
    int i, id, nthreads, start, stop;
    id = omp_get_thread_num();
    nthreads = omp_get_num_threads();
    start = id * N / nthreads;
    stop = (id+1)*N/nthreads;
    if(id==nthreads-1) stop = N;
    for(i=start; i<stop;i++){a[i] = a[i] + b[i];}
 }
 





#pragma omp parallel
#pragma omp for
   for(i=0; i<N;i++){a[i] = a[i] + b[i];}





int j=10;
#pragma omp parallel for
for(i=0; i<N;i++){
    j= j+3;
    a[i] = a[i] + b[j];
}





#pragma omp parallel for
for(i=0; i<N;i++){
    int j= 10+3*(i+1);
    a[i] = a[i] + b[j];
}





#include <omp.h>
#include <stdio.h>
#include <stdlib.h>

int main (int argc, char *argv[]) 
{
int   i, n;
float a[100], b[100], sum; 

/* Some initializations */
n = 100;
for (i=0; i < n; i++)
  a[i] = b[i] = i * 1.0;
sum = 0.0;

#pragma omp parallel for reduction(+:sum)
  for (i=0; i < n; i++)
    sum = sum + (a[i] * b[i]);

printf("   Sum = %f\n",sum);

}


// What reduction does
// - A local copy of each variable for the identifier is made and initialized the 
    shared variable by the identity for the identifier
    

























import cupy as cp
import numpy as np

# Create 2D numpy arrays
a = np.random.random(100000000)
a = a.reshape(10000,10000)

b = np.random.random(100000000)
b = b.reshape(10000,10000)

# Move to GPU
g = cp.asarray(a)
h = cp.asarray(b)

# Matrix Mult
out = cp.matmul(g,h)


print(out)


import os
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1'
import numpy as np
import pandas as pd
import glob
import math
from datetime import date
from functools import partial
import matplotlib.pyplot as plt
from matplotlib import colors
%config InlineBackend.figure_format = 'svg'
plt.rcParams.update({'font.size': '16'})
import warnings; warnings.simplefilter('ignore')



# Define an array of numbers
foo = np.array([0, 1, 2, 3, 4, 5])

# Define a function that squares numbers
def bar(x):
    return x * x

# Loop over each element and perform an action on it
for element in foo:

        # Print the result of bar
        print(bar(element))



# (Very) inefficient way to define a map function
def my_map(function, array):
    # create a container for the results
    output = []

    # loop over each element
    for element in array:
        
        # add the intermediate result to the container
        output.append(function(element))
    
    # return the now-filled container
    return output



my_map(bar, foo)


list(map(bar, foo))

# NB: in python3 `map` is a generator, so we need to cast it to a list for this comparison



import multiprocessing

# Create a pool of processes
with multiprocessing.Pool(processes=6) as pool:
    # map the `np.square` function on our `foo` array
    result = pool.map(np.square, foo)

# output the results
print(result)



def parallel_test(x):
    # print the index of the job and it's process ID number
    s = f"x = {x}, PID = {os.getpid()}"
    print(s)
    return s



list(map(parallel_test, foo));



with multiprocessing.Pool(processes=6) as pool:
    result = pool.map(parallel_test, foo)



fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(5,5))
x = np.linspace(0,1,100)
plt.fill_between(x, np.sqrt(1-x**2),0,alpha=0.1)
plt.xlim(0,1.03);plt.ylim(0,1.03);plt.xlabel('X');plt.ylabel('Y');

x = np.random.random(size=200)
y = np.random.random(size=200)

plt.plot(x,y,marker='.',linestyle='None');



def pi_mc(seed):
    num_trials = 500000
    counter = 0
    np.random.seed(seed)
    
    for j in range(num_trials):
        x_val = np.random.random_sample()
        y_val = np.random.random_sample()

        radius = x_val**2 + y_val**2

        if radius < 1:
            counter += 1
            
    return 4*counter/num_trials


%timeit pi_mc(1)


seed_array = list(range(4))
%timeit list(map(pi_mc, seed_array))



%%timeit

with multiprocessing.Pool(processes=4) as pool:
    result = pool.map(pi_mc, seed_array)





