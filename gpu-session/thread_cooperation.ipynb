{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df1d022",
   "metadata": {},
   "source": [
    "# Thread Cooperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb65d5",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- How CUDA C calls threads.\n",
    "- The mechanism for different threads to communicate with each other.\n",
    "- The mechanism to synchronize the parallel execution of different threads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8c596",
   "metadata": {},
   "source": [
    "###  Splitting Parallel Blocks\n",
    "\n",
    "**For adding two vectors of size N**\n",
    "\n",
    "```cpp\n",
    "add<<<N,1>>>( dev_a, dev_b, dev_c );\n",
    "```\n",
    "N blocks x 1 thread/block = N parallel threads\n",
    "\n",
    "Alternatives - So really, we could have launched N/2 blocks with two threads per block, N/4 blocks with four threads per block.\n",
    "\n",
    "**For changing the parallel block implementation to a parallel thread implementation**\n",
    "\n",
    "```cpp\n",
    "add<<<1,N>>>( dev _ a, dev _ b, dev _ c );\n",
    "```\n",
    "\n",
    "For getting the loop id - \n",
    "```cpp\n",
    "int tid = blockIdx.x;  -> int tid = threadIdx.x;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0bdd9b",
   "metadata": {},
   "source": [
    "#### Vector adition example\n",
    "```cpp\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define HANDLE_ERROR( err )\n",
    "  if ( err != cudaSuccess ) { \n",
    "    fprintf( stderr, \"Error: %s\\n\", cudaGetErrorString( err ) ); \n",
    "    exit( EXIT_FAILURE ); \n",
    "  }\n",
    "\n",
    "#define N 10\n",
    "__global__ void add( int *a, int *b, int *c ) {\n",
    "    int tid = threadIdx.x;\n",
    "    if (tid < N)\n",
    "    c[tid] = a[tid] + b[tid];\n",
    "}\n",
    "\n",
    "int main( void ) {\n",
    "    int a[N], b[N], c[N];\n",
    "    int *dev_a, *dev_b, *dev_c;\n",
    "    // allocate the memory on the GPU\n",
    "    HANDLE_ERROR( cudaMalloc( (void**)&dev_a, N * sizeof(int) ) );\n",
    "    HANDLE_ERROR( cudaMalloc( (void**)&dev_b, N * sizeof(int) ) );\n",
    "    HANDLE_ERROR( cudaMalloc( (void**)&dev_c, N * sizeof(int) ) );\n",
    "    // fill the arrays ‘a’ and ‘b’ on the CPU\n",
    "    for (int i=0; i<N; i++) {\n",
    "        a[i] = i;\n",
    "        b[i] = i * i;\n",
    "    }\n",
    "    // copy the arrays ‘a’ and ‘b’ to the GPU\n",
    "    HANDLE_ERROR( cudaMemcpy( dev_a, a, N * sizeof(int), cudaMemcpyHostToDevice ) );\n",
    "    HANDLE_ERROR( cudaMemcpy( dev_b, b, N * sizeof(int), cudaMemcpyHostToDevice ) );\n",
    "    add<<<1,N>>>( dev_a, dev_b, dev_c );\n",
    "    // copy the array ‘c’ back from the GPU to the CPU\n",
    "    HANDLE_ERROR( cudaMemcpy( c, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost ) );\n",
    "    // display the results\n",
    "    for (int i=0; i<N; i++) {\n",
    "        printf( “%d + %d = %d\\n”, a[i], b[i], c[i] );\n",
    "    }\n",
    "    \n",
    "    // free the memory allocated on the GPU\n",
    "    cudaFree( dev_a );\n",
    "    cudaFree( dev_b );\n",
    "    cudaFree( dev_c );\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e3aa5",
   "metadata": {},
   "source": [
    "#### What if the vector dimension is very big\n",
    "\n",
    "- The hardware limits the number of blocks in a single launch to 65,535\n",
    "- The hardware limits the number of threads per block with which we can launch a kernel - 1024(in our case)\n",
    "- Solution - We will have to use a combination of threads and blocks to accomplish this.\n",
    " \n",
    "```cpp\n",
    "int tid = threadIdx.x + blockIdx.x * blockDim.x\n",
    "```\n",
    "\n",
    "#### What if it is toooooo long\n",
    "\n",
    "- With our current vector implementation, the upper limit of N is 65,535 * 1024\n",
    "- Each thread will start at an index given by the following\n",
    "```cpp \n",
    "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "```\n",
    "- After each thread finishes its work at the current index, we need to increment each of them by the total number of threads running in the grid.\n",
    "- This is simply the number of threads per block multiplied by the number of blocks in the grid, or blockDim.x * gridDim.x\n",
    "```cpp \n",
    "tid += blockDim.x * gridDim.x;\n",
    "```\n",
    "\n",
    "-  To ensure we never launch too many blocks, we will just fix the number of blocks to some reasonably small value. \n",
    "```cpp\n",
    "add<<<128,128>>>( dev _ a, dev _ b, dev _ c );\n",
    "```\n",
    "- Then just change the vector addition function\n",
    "```cpp\n",
    "__global__ void add( int *a, int *b, int *c ) {\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    while (tid < N) {\n",
    "        c[tid] = a[tid] + b[tid];\n",
    "        tid += blockDim.x * gridDim.x;\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd5170",
   "metadata": {},
   "source": [
    "## Shared Memory\n",
    "\n",
    "- The motivation for splitting blocks into threads was simply one of working around hardware limitations to the number of blocks we can have in flight.\n",
    "- CUDA C makes available a region of memory that we call shared memory.\n",
    "- You can use the shared memory in the SM processor\n",
    "    - With keyword\n",
    "    ```cpp\n",
    "     _ _shared_ _\n",
    "    ```\n",
    "- The CUDA C compiler treats variables in shared memory differently than typical variables. \n",
    "- It creates a copy of the variable for each block that you launch on the GPU. \n",
    "- Every thread in that block shares the memory, but threads cannot see or modify the copy of this variable that is seen within other blocks.  \n",
    "- This provides an excellent means by which threads within a block can communicate and collaborate on computations.\n",
    "- If we expect to communicate between threads, we also need a mechanism forsynchronizing between threads. \n",
    "- For example, if thread A writes a value to shared memory and we want thread B to do something with this value, we can’t have thread B start its work until we know the write from thread A is complete. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424d265",
   "metadata": {},
   "source": [
    "### Dot Product example\n",
    "\n",
    "If we take the dot product of two four-element vectors, we would get\n",
    "\n",
    "$(x_1,x_2,x_3,x_4)\\cdot(y_1,y_2,y_3,y_4) = x_1y_1 + x_2y_2+x_3y_3+x_4y_4$\n",
    "\n",
    "- Each thread multiplies a pair of corresponding entries, and then every thread moves on to its next pair.\n",
    "- The result needs to be the sum of all these pairwise products, each thread keeps a running sum of the pairs it has added. \n",
    "- Just like in the addition example, the threads increment their indices by the total number of threads to ensure we don’t miss any elements and don’t multiply a pair twice.\n",
    "\n",
    "\n",
    "```cpp\n",
    "#define imin(a,b) (a<b?a:b)\n",
    "const int N = 33 * 1024;\n",
    "const int threadsPerBlock = 256;\n",
    "__global__ void dot( float *a, float *b, float *c ) {\n",
    "    __shared__ float cache[threadsPerBlock];\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    int cacheIndex = threadIdx.x;\n",
    "    float temp = 0;\n",
    "    while (tid < N) {\n",
    "        temp += a[tid] * b[tid];\n",
    "        tid += blockDim.x * gridDim.x;\n",
    "    }\n",
    "    // set the cache values\n",
    "    cache[cacheIndex] = temp;\n",
    "```\n",
    "\n",
    "- As you can see, we have declared a buffer of shared memory named cache. This buffer will be used to store each thread’s running sum. \n",
    "- It will be possible that not every entry will be used if the size of the input vectors is not a multiple of the number of threads per block.\n",
    "- In this case, the last block will have some threads that do nothing and therefore do not write values.\n",
    "- At this point in the algorithm, we need to sum all the temporary values we’ve placed in the cache.\n",
    "- To do this, we will need some of the threads to read the values that have been stored there.\n",
    "- But we have to make sure this thread will only run after the other threads computes the multiplication\n",
    "\n",
    "```cpp\n",
    "// synchronize threads in this block\n",
    "__syncthreads();\n",
    "\n",
    "```\n",
    "\n",
    "- This call guarantees that every thread in the block has completed instructions prior to the __syncthreads() before the hardware will execute the next instruction on any thread. \n",
    "\n",
    "- We call the general process of taking an input array and performing some computations that produce a smaller array of results a **reduction**\n",
    "\n",
    "- The general idea is that each thread will add two of the values in cache[] and store the result back to cache[]. Since each thread combines two entries into one, we complete this step with half as many entries as we started with. In the next step, we do the same thing on the remaining half.\n",
    "\n",
    "```cpp\n",
    "// for reductions, threadsPerBlock must be a power of 2\n",
    "// because of the following code\n",
    "int i = blockDim.x/2;\n",
    "while (i != 0) {\n",
    "    if (cacheIndex < i)\n",
    "        cache[cacheIndex] += cache[cacheIndex + i];\n",
    "    __syncthreads();\n",
    "    i /= 2;\n",
    "}\n",
    "```\n",
    "**One Step of summation reduction**\n",
    "\n",
    "<img src=\"fig/one_step.png\" width=\"500\"/>\n",
    "\n",
    "- Observe that we update our shared memory buffer cache[] only if cacheIndex is less than i\n",
    "- Since cacheIndex is really just threadIdx.x, this means that only some of the threads are updating entries in the shared memory cache. \n",
    "- Since we are using __syncthreads only to ensure that these updates have taken place before proceeding, it stands to reason that we might see a speed improvement only if we wait for the threads that are actually writing to shared memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfbd3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
