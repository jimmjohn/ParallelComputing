{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernal summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **function type qualifiers:**\n",
    " - $\\_\\_$global$\\_\\_$ is the qualifier for kernels (which can be called from the host and executed on the device).\n",
    "- $\\_\\_$host$\\_\\_$ functions are called from the host and execute on the host. (This is the default qualifier and is often omitted.)\n",
    "- $\\_\\_$device$\\_\\_$ functions are called from the device and execute on the device. (A function that is called from a kernel needs the $\\_\\_$device$\\_\\_$ qualifier.)\n",
    "- Prepending $\\_\\_$host$\\_\\_$ $\\_\\_$device$\\_\\_$ causes the system to compile separate host and device versions of the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels cannot return a value, so the return type is always void, and kernel declarations start as follows:\n",
    "<br>\n",
    "- $\\_\\_$global$\\_\\_$ void aKernel(typedArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of previous discussions\n",
    "\n",
    "- cudaMalloc() allocates device memory\n",
    "- cudaMemcpy() transfers data to or from a device.\n",
    "- cudaFree() frees device memory that is no longer in use.\n",
    "- $\\_\\_$syncThreads() synchronizes threads within a block.- Once all threads have reached this point, execution resumes normally\n",
    "- cudaDeviceSynchronize() effectively synchronizes all threads in a grid.\n",
    "- cudaMallocManaged() - The unified memory relieves you from having to create separate copies of an array (on the host and the device) and from explicitly calling for data transfers between CPU and GPU. Instead, you can create a single managed\n",
    "array that can be accessed from both host and device. In reality, the data in the array needs to be transferred between host and device, but the CUDA system schedules and executes those transfers so you don’t have to.\n",
    "- $\\_\\_$constant$\\_\\_$ - stored in global memory (cached),read-only for threads, written by host\n",
    "- $\\_\\_$shared$\\_\\_$ -stored in shared memory (latency comparable to registers), accessible by all threads in the same threadblock, lifetime: block lifetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication Pattern\n",
    "\n",
    "Let’s brief on the different communication patterns seen in parallel computing. Usually, this is about how\n",
    "to map tasks, and memory together. That is mapping threads in CUDA and the memory that they are\n",
    "communicating through.\n",
    "\n",
    "#### Map\n",
    "\n",
    "- The pattern, map, the program has many data elements: such as the elements of an array, entries in a\n",
    "matrix, or pixels in an image.\n",
    "- Map requires the application of the same function of computation on each piece of data. This means that each thread will read from and write to a specific place in memory\n",
    "\n",
    "#### Gather\n",
    "- Suppose you want to each thread to compute and store the average accross a range of data elements.\n",
    "- Suppose that you want to apply a blur to an image, by setting each pixel to the mean of its neighbouring pixels.\n",
    "- This operation is called gather, because each thread gathers input data elements together from different\n",
    "places to compute an output under some operation.\n",
    "\n",
    "#### Scatter\n",
    "- Now suppose that you want to do the opposite operation. We can have each thread read an input and take\n",
    "a fraction of its value and add it to the neighbouring points as an output result.\n",
    "- When each thread needs to write its output in a different or multiple places we call this the scatter operation.\n",
    "\n",
    "#### Transpose\n",
    "- Transpose is a pattern that can be very useful in array, matrix, image, and data structure manipulation.\n",
    "For example we might have a 2D array, such as an image in row-major order.\n",
    "\n",
    "<img src=\"fig/mat1.png\" width=\"100\"/>\n",
    "\n",
    "In CUDA, the memory is laid as:\n",
    "\n",
    "<img src=\"fig/mat2.png\" width=\"200\"/>\n",
    "\n",
    "It may be advantageous to operate on the columns of the image instead.\n",
    "\n",
    "<img src=\"fig/mat3.png\" width=\"70\"/>\n",
    "\n",
    "So this means we have to reorder the elements in memory as such\n",
    "\n",
    "<img src=\"fig/mat4.png\" width=\"200\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stencil\n",
    "\n",
    "Stencil is a class of algorithms that combines the functionality of map and gather.\n",
    "- 2D von neumann stencil\n",
    "- 2D Moore stencil\n",
    "- 3D von neumann stencil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the following code snippet - Tell which operation it is\n",
    "```cpp\n",
    "float out[], in[];\n",
    "int i = threadIdx.x;\n",
    "int j = threadIdx.y;\n",
    "\n",
    "const float pi = 3.1415;\n",
    "\n",
    "out[i] = pi * in[i];\n",
    "\n",
    "out[i + j*128] = in[j + i*128];\n",
    "\n",
    "if(i%2) {\n",
    "    out[i-1] += pi * in[i]; out[i+1] += pi * in[i];\n",
    "\n",
    "    out[i] = (in[i] + in[i-1] + in[i+1] * pi/ 3.0);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- map - one to one\n",
    "- Transpose - one to one\n",
    "- Gather Many to one\n",
    "- scatter - one to many\n",
    "- stencil - several to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barriers\n",
    "\n",
    "```cpp\n",
    "int i = threadIdx.x;\n",
    "\n",
    "__shared__ int array[256];\n",
    "\n",
    "array[i] = threadIdx.x;\n",
    "__syncThreads();\n",
    "\n",
    "if(i<255) \n",
    "    temp = array[i+1];\n",
    "    __syncThreads();\n",
    "    array[i] = temp;\n",
    "    __syncThreads();\n",
    "\n",
    "```\n",
    "\n",
    "- How many barriers this code needs?\n",
    "\n",
    "\n",
    "```cpp\n",
    "if(i%2) array[i] = array[i-1]   --- Does this need a barrier?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an implicit syncThreads call after each kernal calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - local memory(cache) > shared memory > global memory > cpu host memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False sharing (coalesced vs strided)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atomic memory operation\n",
    "\n",
    "```cpp\n",
    "__global__ void increment(int *g)\n",
    "{\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    i = i%10;\n",
    "    g[i] = g[i]+1;\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution - Implement barrier.\n",
    "\n",
    "Another solution - Atomic memory operation\n",
    "\n",
    "```cpp\n",
    "__global__ void increment_atomin(int *g)\n",
    "{\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    i = i%10;\n",
    "    atomicAdd(&g[i],1);\n",
    "    \n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "**Limitation** - Floating point arithmetic is non-associative, slow because of searalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
